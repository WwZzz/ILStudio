# OpenVLA Fine-tuning Training Configuration
# Optimized for sim_transfer_cube_scripted task

# =============================================================================
# CORE TRAINING PARAMETERS
# =============================================================================

# Training duration
num_train_epochs: 10
max_steps: 5000

# Batch sizes (optimized for OpenVLA 7B with LoRA)
per_device_train_batch_size: 8
per_device_eval_batch_size: 16
gradient_accumulation_steps: 2

# Learning rate and scheduling
learning_rate: 5e-4
weight_decay: 0.01
warmup_steps: 100
warmup_ratio: 0.02
lr_scheduler_type: "cosine"

# Optimizer settings
optim: "adamw_torch"
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8
max_grad_norm: 1.0

# =============================================================================
# OPENVLA SPECIFIC PARAMETERS
# =============================================================================

# LoRA parameters
lora_r: 32
lora_alpha: 64
lora_dropout: 0.1

# Training mode
training_mode: "lora"
use_quantization: false

# Model parameters
max_length: 2048
fp16: true
bf16: false

# =============================================================================
# DATA PROCESSING CONFIGURATION
# =============================================================================

# Data loading
dataloader_num_workers: 4
dataloader_pin_memory: true
remove_unused_columns: false

# Data preprocessing
image_augmentation: true
action_normalization: true
state_normalization: true

# =============================================================================
# LOGGING AND SAVING CONFIGURATION
# =============================================================================

# Logging settings
logging_strategy: "steps"
logging_steps: 10
report_to: "tensorboard"

# Saving settings
save_strategy: "steps"
save_steps: 500
save_total_limit: 10

# Evaluation settings
do_eval: true
eval_strategy: "steps"
eval_steps: 500
evaluation_strategy: "steps"

# =============================================================================
# SYSTEM SETTINGS
# =============================================================================

# Random seed
seed: 42

# Logging directory
logging_dir: "./logs"

# Checkpoint resuming
resume_from_checkpoint: true

# =============================================================================
# OPENVLA SPECIFIC TRAINING ARGUMENTS
# =============================================================================

# Action prediction specific
action_loss_weight: 1.0
state_loss_weight: 0.1
language_loss_weight: 0.1

# Image processing
image_size: [480, 640]
camera_names: ["primary"]

# Task specific
task_name: "sim_transfer_cube_scripted"
action_dim: 14
state_dim: 14

# =============================================================================
# MEMORY OPTIMIZATION
# =============================================================================

# Gradient checkpointing
gradient_checkpointing: true

# Memory efficient attention
use_memory_efficient_attention: true

# =============================================================================
# WANDB CONFIGURATION (Optional)
# =============================================================================

# Weights & Biases logging
wandb_project: "openvla-sim-transfer-cube"
wandb_entity: null
wandb_run_name: "openvla-7b-lora-finetune"

# =============================================================================
# DEBUGGING AND MONITORING
# =============================================================================

# Debug mode
debug_mode: false

# Monitoring
monitor_training: true
early_stopping_patience: 3
early_stopping_threshold: 0.01

# =============================================================================
# ADVANCED TRAINING SETTINGS
# =============================================================================

# Mixed precision training
fp16_full_eval: false

# Dataloader settings
dataloader_drop_last: true

# Evaluation settings
eval_accumulation_steps: 1
eval_delay: 0

# =============================================================================
# OPENVLA SPECIFIC ADVANCED SETTINGS
# =============================================================================

# Action tokenization
action_tokenization_method: "continuous"
action_bins: 256

# Vision processing
vision_backbone_freeze: false
vision_backbone_lr_multiplier: 0.1

# Language processing
language_model_freeze: false
language_model_lr_multiplier: 1.0

# Multi-modal fusion
fusion_method: "cross_attention"
fusion_layers: [4, 8, 12, 16]
